# ELMo的初步探索
## Deep contextualized word representations

---

### **摘要**
- 论文提出了一种新的“深度上下文相关词表征”ELMo，它能建模词汇的复杂特征和在不同语境中的多义性。
- ELMo通过预训练的深度双向语言模型生成词向量，并能轻松提升多种NLP任务（如情感分析）的性能。
- 论文的核心创新在于利用模型的所有内部层，而不仅仅是最后一层。

### **相关工作**
- 早期预训练词向量是上下文无关的，后续工作通过子词信息或多义词向量进行了改进。
- 而同时期其他上下文相关表征方法（如context2vec, CoVe）在数据规模或模型深度上存在局限。
- 研究表明，深度循环网络的不同层编码不同信息（如底层偏向句法，高层偏向语义），这为ELMo利用所有层提供了依据。
 
### **ELMo: 来自语言模型的嵌入**
- **双向语言模型**：联合训练前向和后向语言模型，共同预测下一个词和上一个词。
- **ELMo**：ELMo向量是双向语言模型各层输出的任务特定加权和，而非仅使用顶层。
- **用于监督NLP任务**：将ELMo向量作为额外特征，与静态词嵌入拼接后输入下游任务模型，有时在模型输出层加入效果更佳。
- **预训练架构**：描述了具体的模型结构（2层BiLSTM、字符卷积等）和训练细节（在1B Word Benchmark上训练）。

### **评估**
- 在六项NLP任务（问答、文本蕴含、语义角色标注、共指消解、命名实体识别、情感分析）中加入ELMo，均显著提升了当时的最高水平。
- 所有任务都获得了6%-20%的相对错误率下降，其中在情感分析任务上，ELMo替换CoVe带来了1.0%的准确率绝对提升。

### **分析**
- **层加权方案**：使用所有层（并通过学习权重加权）比仅使用顶层效果更好。
- **ELMo的加入位置**：根据不同任务架构（如有无注意力层），将ELMo同时加入模型输入和输出层可能获得额外提升。
- **BiLM捕捉的信息**：高层表征更偏向词义消歧等语义信息，底层则更偏向词性标注等句法信息。
- **样本效率**：加入ELMo能大幅减少模型达到最优性能所需的训练数据和迭代次数。
- **可视化权重**：输入层通常更依赖底层句法信息，输出层的权重则相对均衡。

### **结论**
- ELMo提供了一种从双向语言模型学习高质量深度上下文表征的通用方法。
- 通过利用所有层，ELMo有效编码了词汇的句法和语义信息。
- 该方法在广泛的NLP任务上带来了巨大性能提升。

---

### 核心思想
- 通过双向语言模型预训练获取深度的、上下文相关的词向量，并以一个可学习的线性组合方式利用所有层的输出，以此捕捉从底层句法到高层语义的丰富信息，从而显著提升下游任务性能。
### 模型结构
- 目标：用于解决词向量的动态表示问题，根据不同的上下文，同一个词能有不同的表示。
- ELMo采用的是L层BiLSTM的模型结构（当L = 2时）
  - 最底层的E_i是单词索引token_i经过编码层产生的词向量；
  - 中间是L层BiLSTM结构，用来提取上下文特征，它由一个前向和一个后向语言模型构成，目标函数就是取这两个方向语言模型的最大似然；
     - 前向语言模型：利用前面的信息估计后面的信息，第k个单词出现的概率依赖于前面k - 1个单词：就是条件概率公式的连乘形式
     - 后向语言模型：利用后面的信息估计前面的信息，第k个单词出现的概率依赖于后面n - k个单词：同样条件概率公式的连乘形式
     - 目标函数：整合上面两种语言模型，最大化对数似然函数
  - 同一个单词经过L层BiLSTM所产生的前向向量和后向向量经过组合会产生后面的T_i
### 训练方法
- 对于每一个单词token_i，最终会有2L+1个向量表示，2L是L层BiLSTM产生的隐向量，剩下的1是最初单词从索引映射的向量；
- 将这2L+1个embedding压缩成一个embedding来进行不同任务下的预训练；
- 得到最终的压缩表示，就可以完成第一阶段的语言模型预训练了
### 使用方法
- 为了将词向量应用到别的NLP任务中，可以将固定权重，然后将最初的CNN编码表示和压缩表示拼接，然后接入到别的任务；
- 为了提高预训练模型效果，可以引入dropout和2阶正则项
### 局限性
- BiLSTM效率低，且在长距离依赖问题上表现不佳
- 提取特征的能力还是比较弱
- 拼接方式双向融合特征融合能力比较弱


---


